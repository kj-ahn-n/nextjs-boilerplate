import type { NextApiRequest, NextApiResponse } from 'next';
import axios from 'axios';
import * as cheerio from 'cheerio';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
    try {
        let url = "https://redditinc.com/blog"; // ì‹œì‘ í˜ì´ì§€
        const articles: { title: string; link: string; date: string }[] = [];
        let hasNextPage = true;
        let pageNumber = 1; // í˜ì´ì§€ ìˆ˜ ì¶”ì 

        while (hasNextPage && pageNumber <= 10) { // ìµœëŒ€ 10í˜ì´ì§€ê¹Œì§€ í¬ë¡¤ë§
            console.log(`ğŸ” Scraping page ${pageNumber}: ${url}`);
            const { data } = await axios.get(url);
            const $ = cheerio.load(data);

            // í˜„ì¬ í˜ì´ì§€ì˜ ê¸€ í¬ë¡¤ë§
            $(".entry-header").each((_, element) => {
                const titleElement = $(element).find("h2.entry-title a");
                const dateElement = $(element).find("time.entry-date");

                const title = titleElement.text().trim();
                const link = titleElement.attr("href") || "";
                const date = dateElement.text().trim();

                if (title && link && date) {
                    articles.push({ title, link, date });
                }
            });

            // ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ì°¾ê¸°
            const nextPageElement = $(".hs-pagination__link--next");
            const nextPage = nextPageElement.attr("href");

            // ë‹¤ìŒ í˜ì´ì§€ê°€ ì—†ê±°ë‚˜ 10í˜ì´ì§€ë¥¼ ì´ˆê³¼í•˜ë©´ ì¢…ë£Œ
            console.log('pageNumber: ', pageNumber);
            if (!nextPage || pageNumber >= 10) {
                hasNextPage = false;
                break;
            }

            // ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
            url = nextPage;
            pageNumber++; // í˜ì´ì§€ ì¦ê°€

            // ë”œë ˆì´ ì¶”ê°€ (ëœë¤ 2~5ì´ˆ)
            const delay = Math.floor(Math.random() * (5000 - 2000 + 1)) + 2000;
            console.log(`â³ Waiting ${delay / 1000} seconds before next page...`);
            await new Promise(resolve => setTimeout(resolve, delay));
        }

        console.log("âœ… Finished scraping up to 10 pages!");
        res.status(200).json({ articles });
    } catch (error) {
        console.error("âŒ Scraping failed:", error);
        res.status(500).json({ error: "Scraping failed" });
    }
}